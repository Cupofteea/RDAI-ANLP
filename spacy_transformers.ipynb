{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install relevant packages if not done so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"forum_breached_20221115_20221201_155_annotations.jsonl\"\n",
    "file2 = \"forum_exploit_20220101_20220201_300_posts_set1_226_annotations.jsonl\"\n",
    "file3 = \"forum_exploit_20220301_20220401_251_annotations.jsonl\"\n",
    "file4 =  \"forum_exploit_20220801_20220815_163_annotations.jsonl\"\n",
    "file5 = \"forum_nulled_20220801_20220815_147_annotations.jsonl\"\n",
    "file6 = \"forum_xss_posts_20220801_20220815_157_annotations.jsonl\"\n",
    "file7 = \"popular_forums_20221101_20221104_500_posts_set1_486_annotations.jsonl\"\n",
    "phishing = \"phishing.jsonl\"\n",
    "company_orginfo = \"company_orginfo.jsonl\"\n",
    "vuln = \"vulnerability.jsonl\"\n",
    "\n",
    "files = [file1,file2,file3,file4,file5,file6,file7,phishing,company_orginfo,vuln]\n",
    "\n",
    "#Importing relevant files\n",
    "\n",
    "def merge_jsonl_files(files):\n",
    "    curr_path = os.getcwd()\n",
    "    df_list = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(curr_path,\"..\",\"prodigy\",\"annotation_output\", file)\n",
    "        print(file)\n",
    "        print(file_path)\n",
    "        df = pd.read_json(file_path,lines= True)\n",
    "        df_list.append(df)\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "#Removing special characters\n",
    "def sp_char_remove(review):\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    return review\n",
    "\n",
    "#Removing special characters\n",
    "def stopword_remover(text):\n",
    "    x=[]\n",
    "    text=text.split()    #splitting into individual words\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english'):\n",
    "            x.append(i)\n",
    "    return x\n",
    "\n",
    "def url_remover(text):\n",
    "    remove = \"http\\S+\"\n",
    "    text = re.sub(remove, \" \", text)\n",
    "    return text\n",
    "\n",
    "#Total dataframe\n",
    "df_dummy = merge_jsonl_files(files)\n",
    "df_dummy_dummy = df_dummy[df_dummy.answer == \"accept\"]\n",
    "df = df_dummy_dummy.drop(columns=[\"_input_hash\",\"_session_id\",\"_task_hash\",\"_view_id\",\"options\",\"config\", \"answer\"])\n",
    "#df[\"accept\"] = df[\"accept\"].apply(lambda x: x if x else [\"EMPTY\"])\n",
    "df[\"text\"] = df[\"text\"].apply(url_remover)\n",
    "#df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "del df[\"meta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labelled outputs to a binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "label_path = os.path.join(cwd, \"..\",\"labels.txt\")\n",
    "label_data = open(label_path,\"r\").read()\n",
    "labels = label_data.split(\"\\n\")\n",
    "mlb = MultiLabelBinarizer(classes=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting df to training and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df to training and test\n",
    "train, validation = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"size of training data:\",len(train))\n",
    "print(\"size of test data:\", len(validation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_bin_format(nlp, row, categories):\n",
    "    doc = nlp.make_doc(row[\"text\"])\n",
    "    #print(categories)\n",
    "    doc.cats = {cat: 0 for cat in categories}\n",
    "\n",
    "    for label in row[\"accept\"]:\n",
    "        doc.cats[label] = 1\n",
    "    #print(doc.cats)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DocBin - train\n",
    "num_of_rows = len(train)\n",
    "docs = []\n",
    "categories = mlb.classes\n",
    "\n",
    "for i in range(num_of_rows):\n",
    "    row = train.iloc[i]\n",
    "    doc = convert_text_to_bin_format(nlp, row, categories)\n",
    "    docs.append(doc)\n",
    "\n",
    "train_doc_bin = DocBin(docs=docs)\n",
    "curr_path = os.getcwd()\n",
    "path = os.path.join(curr_path,\"..\",\"data\",\"transformer_data_no_url\",\"training.spacy\")\n",
    "\n",
    "train_doc_bin.to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DocBin - validation\n",
    "num_of_rows = len(validation)\n",
    "docs = []\n",
    "categories = mlb.classes\n",
    "for i in range(num_of_rows):\n",
    "    row = validation.iloc[i]\n",
    "    doc = convert_text_to_bin_format(nlp, row, categories)\n",
    "    docs.append(doc)\n",
    "\n",
    "test_doc_bin = DocBin(docs=docs)\n",
    "curr_path = os.getcwd()\n",
    "path = os.path.join(curr_path,\"..\",\"data\",\"transformer_data_no_url\",\"validation.spacy\")\n",
    "\n",
    "test_doc_bin.to_disk(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating specific spacy config files for training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m spacy init fill-config <path/to/input/base/config/file>  <output/config/path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config configs/base_config_textcat.cfg configs/txt_classification_config_batch128_raw.cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m spacy train <path/of/config/file> --output <path/to/save/model> --paths.train <training/data/path> --paths.dev <validation/data/path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/txt_classification_config.cfg --output ../models/v4 --paths.train ./../data/transformer_data_no_url/training.spacy --paths.dev ./../data/transformer_data_no_url/validation.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_tags(doc, threshold):\n",
    "    tags = doc.cats\n",
    "    for k in tags:\n",
    "        #print(k)\n",
    "        if tags[k] >= threshold:\n",
    "            tags[k] = 1\n",
    "        else:\n",
    "            tags[k] = 0\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_nlp = spacy.load(\"models/trf_output/model-best\")\n",
    "text = \" 685K HQ Private Combolist Email:Pass [Netflix,Minecraft,Uplay,Steam,Paypal,Hulu,Vpn,Spotify,Etc....]  PLZ REPLY THIS THREAD FOR MOR COMBO  Download Link......   https://rosefile.net/2m9km2ui7g/685K.zip.html 685K HQ Private Combolist Email:Pass [Netflix,Minecraft,Uplay,Steam,Paypal,Hulu,Vpn,Spotify,Etc....]\\n\\nPLZ REPLY THIS THREAD FOR MOR COMBO\\n\\nDownload Link......\\n\\nHidden Content\\n\\nReply  to this topic to view hidden content or  Update your account. (https://crackingall.com/index.php?/subscriptions/)\"\n",
    "doc = trained_nlp(text)\n",
    "doc.cats\n",
    "#print(standardize_tags(doc, 0.9))\n",
    "#doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Invicti Vulnerability Scanner Professional version. Windows version. I tested it and it works fine. \\n\\nNot spywares, not backdoors or any type of bullshit. Now, as always, you should try this in your virtual machine and all that.\\n\\nInstruction to use this:\\n\\nYou just have to find the file files in the folder and the main one is\\u00a0Netsparker.exe. Its pretty easy to use but if anything, you should try googling etc. I is important that you allow this in your firewall list of programs etc. This is a version for windows.\\n\\nI tasted it against some vulnerable and non vulnerable web app that I setup on my machine\\u00a0and it works.\\n\\nDisclaimer: This is for educational purposes only and you should always have permission to perform the scanning etc. I don't take any responsibility for stupid skidoos\\n\\nand Mr robot fans.\\n\\nI wanted to share this without credits but I feel that leachers are dangerous and usually they use the tools to cause damage and do not contribute at all.\\n\\nEnjoy\\n\\nHidden Content\\n\\nYou must reply to this thread to view this content.\\n\\nHidden Content\\n\\nYou must reply to this thread to view this content.\"\n",
    "doc = trained_nlp(text)\n",
    "print(standardize_tags(doc, 0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f4ae5743a3c975258dd28a1e83587b05345ccc825b0fe1b5846a6cb0700706e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
