{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDAI-ANLP Project\n",
    "In this project, I am going to do a multi-label classification of text collected from the dark-web forums. There are a total of 24 labels and 1.9k texts training set + 252 texts test set. All the data collected are custom scraped and labelled because there are little or no labelled datasets of the dark web forums into the respectives categories. These data and labels are inclined towards the cyber domain. Labels include data leaks, personal information, company or organisation information etc.\n",
    "\n",
    "I will be using spacy (Word2vec + roberta) to train a model that can classify these texts into the relevant categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REQUEST FOR SERVICE OR PRODUCT',\n",
       " 'OFFERING OF SERVICE OR PRODUCT',\n",
       " 'MONEY INVOLVED',\n",
       " 'ADVICE',\n",
       " 'NETWORK OR PANEL ACCESS',\n",
       " 'CREDENTIALS OR ACCOUNTS',\n",
       " 'CARDING',\n",
       " 'INFRASTRUCTURE AND HOSTING',\n",
       " 'DATA LEAKS',\n",
       " 'PERSONAL INFORMATION',\n",
       " 'COMPANY OR ORG INFORMATION',\n",
       " 'ADULT',\n",
       " 'MALWARE TOOLS AND EXPLOITS',\n",
       " 'VULNERABILITY',\n",
       " 'RECRUITMENT',\n",
       " 'DEFACEMENT',\n",
       " 'PHISHING',\n",
       " 'SPAMMING',\n",
       " 'HACKING',\n",
       " 'SCAM PAGE',\n",
       " 'LOGS',\n",
       " 'SMS OR EMAIL MAILER',\n",
       " 'GOOD REVIEW',\n",
       " 'BAD REVIEW']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "label_path = os.path.join(cwd, \"labels.txt\")\n",
    "label_data = open(label_path,\"r\").read()\n",
    "labels = label_data.split(\"\\n\")\n",
    "mlb = MultiLabelBinarizer(classes=labels)\n",
    "mlb.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant training files\n",
    "I prevoiusly have trained and pre-processing on the texts such as removing stopwords, converting all to lowercase. But after several iterations and comparisons of results, I have decided to feed the raw text directly into the model because of how messy and unstructued texts are in the dark web forums. I also believe that the unstructuredness would hold valuable information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_breached_20221115_20221201_155_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_exploit_20220101_20220201_300_posts_set1_226_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_exploit_20220301_20220401_251_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_exploit_20220801_20220815_163_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_nulled_20220801_20220815_147_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_xss_posts_20220801_20220815_157_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/popular_forums_20221101_20221104_500_posts_set1_486_annotations.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/phishing.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/company_orginfo.jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/vulnerability.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>accept</th>\n",
       "      <th>_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Government of San Pedro Garza Garcia, NL, Mexi...</td>\n",
       "      <td>[DATA LEAKS, PERSONAL INFORMATION]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECUADOR CELL PHONE WHATSAPP DATABASE ECUADOR C...</td>\n",
       "      <td>[DATA LEAKS, PERSONAL INFORMATION]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i need to buy combo virgilio.it with high mail...</td>\n",
       "      <td>[REQUEST FOR SERVICE OR PRODUCT, MONEY INVOLVE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x2100 Fresh Logs [9.11.2022] World(USA, EU inc...</td>\n",
       "      <td>[LOGS, OFFERING OF SERVICE OR PRODUCT, DATA LE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nirvana - Smells Like Teen Spirit ( \\n\\nRemast...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Acunetix Version 1.7.1.955 (Vulnerability Data...</td>\n",
       "      <td>[VULNERABILITY]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Acunetix Version 1.7.1.955 (Vulnerability Data...</td>\n",
       "      <td>[VULNERABILITY]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Twitter (Partial) Database - Leaked, Download!...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, DATA LEAKS, C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Обсуждение  Спутниковый интернет Раз тема каса...</td>\n",
       "      <td>[VULNERABILITY, MALWARE TOOLS AND EXPLOITS, AD...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>List of SQL Injection vulnerable sites (commen...</td>\n",
       "      <td>[ADVICE, MALWARE TOOLS AND EXPLOITS, VULNERABI...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Government of San Pedro Garza Garcia, NL, Mexi...   \n",
       "1    ECUADOR CELL PHONE WHATSAPP DATABASE ECUADOR C...   \n",
       "2    i need to buy combo virgilio.it with high mail...   \n",
       "3    x2100 Fresh Logs [9.11.2022] World(USA, EU inc...   \n",
       "4    Nirvana - Smells Like Teen Spirit ( \\n\\nRemast...   \n",
       "..                                                 ...   \n",
       "250  Acunetix Version 1.7.1.955 (Vulnerability Data...   \n",
       "251  Acunetix Version 1.7.1.955 (Vulnerability Data...   \n",
       "252  Twitter (Partial) Database - Leaked, Download!...   \n",
       "253  Обсуждение  Спутниковый интернет Раз тема каса...   \n",
       "254  List of SQL Injection vulnerable sites (commen...   \n",
       "\n",
       "                                                accept  _timestamp  \n",
       "0                   [DATA LEAKS, PERSONAL INFORMATION]         NaN  \n",
       "1                   [DATA LEAKS, PERSONAL INFORMATION]         NaN  \n",
       "2    [REQUEST FOR SERVICE OR PRODUCT, MONEY INVOLVE...         NaN  \n",
       "3    [LOGS, OFFERING OF SERVICE OR PRODUCT, DATA LE...         NaN  \n",
       "4                                                   []         NaN  \n",
       "..                                                 ...         ...  \n",
       "250                                    [VULNERABILITY]         NaN  \n",
       "251                                    [VULNERABILITY]         NaN  \n",
       "252  [OFFERING OF SERVICE OR PRODUCT, DATA LEAKS, C...         NaN  \n",
       "253  [VULNERABILITY, MALWARE TOOLS AND EXPLOITS, AD...         NaN  \n",
       "254  [ADVICE, MALWARE TOOLS AND EXPLOITS, VULNERABI...         NaN  \n",
       "\n",
       "[1902 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = \"forum_breached_20221115_20221201_155_annotations.jsonl\"\n",
    "file2 = \"forum_exploit_20220101_20220201_300_posts_set1_226_annotations.jsonl\"\n",
    "file3 = \"forum_exploit_20220301_20220401_251_annotations.jsonl\"\n",
    "file4 =  \"forum_exploit_20220801_20220815_163_annotations.jsonl\"\n",
    "file5 = \"forum_nulled_20220801_20220815_147_annotations.jsonl\"\n",
    "file6 = \"forum_xss_posts_20220801_20220815_157_annotations.jsonl\"\n",
    "file7 = \"popular_forums_20221101_20221104_500_posts_set1_486_annotations.jsonl\"\n",
    "phishing = \"phishing.jsonl\"\n",
    "company_orginfo = \"company_orginfo.jsonl\"\n",
    "vuln = \"vulnerability.jsonl\"\n",
    "\n",
    "files = [file1,file2,file3,file4,file5,file6,file7,phishing,company_orginfo,vuln]\n",
    "\n",
    "#Importing relevant files\n",
    "\n",
    "def merge_jsonl_files(files):\n",
    "    curr_path = os.getcwd()\n",
    "    df_list = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(curr_path,\"prodigy\",\"annotation_output\", file)\n",
    "        print(file_path)\n",
    "        df = pd.read_json(file_path,lines= True)\n",
    "        df_list.append(df)\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "#Removing special characters\n",
    "def sp_char_remove(review):\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    return review\n",
    "\n",
    "#Removing special characters\n",
    "def stopword_remover(text):\n",
    "    x=[]\n",
    "    text=text.split()    #splitting into individual words\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english'):\n",
    "            x.append(i)\n",
    "    return x\n",
    "\n",
    "def url_remover(text):\n",
    "    remove = \"http\\S+\"\n",
    "    text = re.sub(remove, \" \", text)\n",
    "    return text\n",
    "\n",
    "#Total dataframe\n",
    "df_dummy = merge_jsonl_files(files)\n",
    "df_dummy_dummy = df_dummy[df_dummy.answer == \"accept\"]\n",
    "df = df_dummy_dummy.drop(columns=[\"_input_hash\",\"_session_id\",\"_task_hash\",\"_view_id\",\"options\",\"config\", \"answer\"])\n",
    "#df[\"accept\"] = df[\"accept\"].apply(lambda x: x if x else [\"EMPTY\"])\n",
    "#df[\"text\"] = df[\"text\"].apply(url_remover)\n",
    "#df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "del df[\"meta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>accept</th>\n",
       "      <th>_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Government of San Pedro Garza Garcia, NL, Mexi...</td>\n",
       "      <td>[DATA LEAKS, PERSONAL INFORMATION]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECUADOR CELL PHONE WHATSAPP DATABASE ECUADOR C...</td>\n",
       "      <td>[DATA LEAKS, PERSONAL INFORMATION]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i need to buy combo virgilio.it with high mail...</td>\n",
       "      <td>[REQUEST FOR SERVICE OR PRODUCT, MONEY INVOLVE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x2100 Fresh Logs [9.11.2022] World(USA, EU inc...</td>\n",
       "      <td>[LOGS, OFFERING OF SERVICE OR PRODUCT, DATA LE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nirvana - Smells Like Teen Spirit ( \\n\\nRemast...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Government of San Pedro Garza Garcia, NL, Mexi...   \n",
       "1  ECUADOR CELL PHONE WHATSAPP DATABASE ECUADOR C...   \n",
       "2  i need to buy combo virgilio.it with high mail...   \n",
       "3  x2100 Fresh Logs [9.11.2022] World(USA, EU inc...   \n",
       "4  Nirvana - Smells Like Teen Spirit ( \\n\\nRemast...   \n",
       "\n",
       "                                              accept  _timestamp  \n",
       "0                 [DATA LEAKS, PERSONAL INFORMATION]         NaN  \n",
       "1                 [DATA LEAKS, PERSONAL INFORMATION]         NaN  \n",
       "2  [REQUEST FOR SERVICE OR PRODUCT, MONEY INVOLVE...         NaN  \n",
       "3  [LOGS, OFFERING OF SERVICE OR PRODUCT, DATA LE...         NaN  \n",
       "4                                                 []         NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of dataset and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text\n",
      "i need to buy combo virgilio.it with high mail access telegram contact: @Nabuto1\n",
      "==================\n",
      "Sample labels\n",
      "['REQUEST FOR SERVICE OR PRODUCT', 'MONEY INVOLVED', 'NETWORK OR PANEL ACCESS']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text\")\n",
    "print(df[\"text\"].tolist()[2])\n",
    "print(\"==================\")\n",
    "print(\"Sample labels\")\n",
    "print(df[\"accept\"].tolist()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text\n",
      "Will @IntelBroker become bigger than @KelvinSecurity Looking on the breaches @ IntelBroker (   is doing in the past month and the rate he is doing them. Do you think he will soon be one of the big names. \n",
      "\n",
      "Like \n",
      "\n",
      "@ LeakBase (  \n",
      "\n",
      "@ kelvinsecurity ( \n",
      "==================\n",
      "Sample labels\n",
      "['ADVICE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text\")\n",
    "print(df[\"text\"].tolist()[10])\n",
    "print(\"==================\")\n",
    "print(\"Sample labels\")\n",
    "print(df[\"accept\"].tolist()[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text\n",
      "Nirvana - Smells Like Teen Spirit ( \n",
      "\n",
      "Remastered in HD, Enjoy\n",
      "\n",
      "  \n",
      "\n",
      " Thanks  @ SafeSig (   for the credits and VIP rank\n",
      "==================\n",
      "Sample labels\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Example of a text that does not fit into any category\n",
    "print(\"Sample text\")\n",
    "print(df[\"text\"].tolist()[4])\n",
    "print(\"==================\")\n",
    "print(\"Sample labels\")\n",
    "print(df[\"accept\"].tolist()[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting df to training and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df to training and test\n",
    "train, validation = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"size of training data:\",len(train))\n",
    "print(\"size of test data:\", len(validation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DocBin\n",
    "this is necsarry as DocBin is one of spaCy accepted input formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_bin_format(nlp, row, categories):\n",
    "    doc = nlp.make_doc(row[\"text\"])\n",
    "    #print(categories)\n",
    "    doc.cats = {cat: 0 for cat in categories}\n",
    "\n",
    "    for label in row[\"accept\"]:\n",
    "        doc.cats[label] = 1\n",
    "    #print(doc.cats)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DocBin - train\n",
    "num_of_rows = len(train)\n",
    "docs = []\n",
    "categories = mlb.classes\n",
    "\n",
    "for i in range(num_of_rows):\n",
    "    row = train.iloc[i]\n",
    "    doc = convert_text_to_bin_format(nlp, row, categories)\n",
    "    docs.append(doc)\n",
    "\n",
    "train_doc_bin = DocBin(docs=docs)\n",
    "curr_path = os.getcwd()\n",
    "path = os.path.join(curr_path,\"..\",\"data\",\"training.spacy\")\n",
    "\n",
    "train_doc_bin.to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DocBin - validation\n",
    "num_of_rows = len(validation)\n",
    "docs = []\n",
    "categories = mlb.classes\n",
    "for i in range(num_of_rows):\n",
    "    row = validation.iloc[i]\n",
    "    doc = convert_text_to_bin_format(nlp, row, categories)\n",
    "    docs.append(doc)\n",
    "\n",
    "test_doc_bin = DocBin(docs=docs)\n",
    "curr_path = os.getcwd()\n",
    "path = os.path.join(curr_path,\"..\",\"data\",\"validation.spacy\")\n",
    "\n",
    "test_doc_bin.to_disk(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating specific spacy config files for training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m spacy init fill-config <path/to/input/base/config/file>  <output/config/path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config configs/base_config_textcat.cfg configs/txt_classification_config_batch128_raw.cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m spacy train <path/of/config/file> --output <path/to/save/model> --paths.train <training/data/path> --paths.dev <validation/data/path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/txt_classification_config.cfg --output ../models/v3 --paths.train ./../data/training.spacy --paths.dev ./../data/validation.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_tags(doc, threshold):\n",
    "    tags = doc.cats\n",
    "    for k in tags:\n",
    "        #print(k)\n",
    "        if tags[k] >= threshold:\n",
    "            tags[k] = 1\n",
    "        else:\n",
    "            tags[k] = 0\n",
    "    return tags\n",
    "\n",
    "# Using model to predict each text.\n",
    "def nlp_predict(text, nlp, threshold):\n",
    "    doc = nlp(text)\n",
    "    tags = standardize_tags(doc, threshold)\n",
    "    tags_list = []\n",
    "    for k,v in tags.items():\n",
    "        if v == 1:\n",
    "            tags_list.append(k)\n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'REQUEST FOR SERVICE OR PRODUCT': 0, 'OFFERING OF SERVICE OR PRODUCT': 1, 'MONEY INVOLVED': 0, 'ADVICE': 0, 'NETWORK OR PANEL ACCESS': 0, 'CREDENTIALS OR ACCOUNTS': 1, 'CARDING': 0, 'INFRASTRUCTURE AND HOSTING': 0, 'DATA LEAKS': 1, 'PERSONAL INFORMATION': 0, 'COMPANY OR ORG INFORMATION': 0, 'ADULT': 0, 'MALWARE TOOLS AND EXPLOITS': 0, 'VULNERABILITY': 0, 'RECRUITMENT': 0, 'DEFACEMENT': 0, 'PHISHING': 0, 'SPAMMING': 0, 'HACKING': 0, 'SCAM PAGE': 0, 'LOGS': 0, 'SMS OR EMAIL MAILER': 0, 'GOOD REVIEW': 0, 'BAD REVIEW': 0}\n"
     ]
    }
   ],
   "source": [
    "trained_nlp = spacy.load(\"v3/model-best\")\n",
    "text = \" 685K HQ Private Combolist Email:Pass [Netflix,Minecraft,Uplay,Steam,Paypal,Hulu,Vpn,Spotify,Etc....]  PLZ REPLY THIS THREAD FOR MOR COMBO  Download Link......   https://rosefile.net/2m9km2ui7g/685K.zip.html 685K HQ Private Combolist Email:Pass [Netflix,Minecraft,Uplay,Steam,Paypal,Hulu,Vpn,Spotify,Etc....]\\n\\nPLZ REPLY THIS THREAD FOR MOR COMBO\\n\\nDownload Link......\\n\\nHidden Content\\n\\nReply  to this topic to view hidden content or  Update your account. (https://crackingall.com/index.php?/subscriptions/)\"\n",
    "doc = trained_nlp(text)\n",
    "doc.cats\n",
    "print(standardize_tags(doc, 0.9))\n",
    "# I am expecting the following labels: Data Leaks, offering service or product, credentials or accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'REQUEST FOR SERVICE OR PRODUCT': 0, 'OFFERING OF SERVICE OR PRODUCT': 0, 'MONEY INVOLVED': 0, 'ADVICE': 1, 'NETWORK OR PANEL ACCESS': 1, 'CREDENTIALS OR ACCOUNTS': 0, 'CARDING': 0, 'INFRASTRUCTURE AND HOSTING': 0, 'DATA LEAKS': 0, 'PERSONAL INFORMATION': 0, 'COMPANY OR ORG INFORMATION': 0, 'ADULT': 0, 'MALWARE TOOLS AND EXPLOITS': 0, 'VULNERABILITY': 0, 'RECRUITMENT': 0, 'DEFACEMENT': 0, 'PHISHING': 0, 'SPAMMING': 0, 'HACKING': 1, 'SCAM PAGE': 0, 'LOGS': 0, 'SMS OR EMAIL MAILER': 0, 'GOOD REVIEW': 0, 'BAD REVIEW': 0}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi guys!!!!! I am planning to exploit this particular CVE to gain remote access to the company's infrastrcuture! do u think this will help??\"\n",
    "doc = trained_nlp(text)\n",
    "doc.cats\n",
    "print(standardize_tags(doc, 0.8))\n",
    "\n",
    "# I am expecting the following labels: Advice. network or panel access, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_nulled_20230123_20230207_61_annotations_(test_set).jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_breached_20230123_20230207_102_annotations_(test_set).jsonl\n",
      "/home/seb/Desktop/Seb/RDAI/ANLP/RDAI-ANLP/prodigy/annotation_output/forum_xss_20230123_20230207_105_annotations_(test_set).jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>accept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x2000 Steam Accounts with Games #4  \\n\\nThis l...</td>\n",
       "      <td>[DATA LEAKS, CREDENTIALS OR ACCOUNTS, OFFERING...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332K Combolist EDU OFFICE 332K Combolist EDU O...</td>\n",
       "      <td>[DATA LEAKS, COMPANY OR ORG INFORMATION, OFFER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mycanal ACCOUNTS PREMIUM diariatouaidara1999@g...</td>\n",
       "      <td>[CREDENTIALS OR ACCOUNTS, DATA LEAKS, OFFERING...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Connecting to shoutbox Anyone have solution to...</td>\n",
       "      <td>[ADVICE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BWW - Free Food - Accounts with Over 1000 Pts ...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Looking for stealer Hello please what’s the la...</td>\n",
       "      <td>[REQUEST FOR SERVICE OR PRODUCT, ADVICE, MALWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Android vulnerability to install a silent payl...</td>\n",
       "      <td>[VULNERABILITY, MALWARE TOOLS AND EXPLOITS, AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Email/Phone leads - USA banks I have variously...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, MONEY INVOLVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110K Malaysian Online Casino Customers [ Depos...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, DATA LEAKS, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>PLEASE DELETE</td>\n",
       "      <td>[DEFACEMENT, GOOD REVIEW, SCAM PAGE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    x2000 Steam Accounts with Games #4  \\n\\nThis l...   \n",
       "1    332K Combolist EDU OFFICE 332K Combolist EDU O...   \n",
       "2    Mycanal ACCOUNTS PREMIUM diariatouaidara1999@g...   \n",
       "4    Connecting to shoutbox Anyone have solution to...   \n",
       "5    BWW - Free Food - Accounts with Over 1000 Pts ...   \n",
       "..                                                 ...   \n",
       "101  Looking for stealer Hello please what’s the la...   \n",
       "102  Android vulnerability to install a silent payl...   \n",
       "103  Email/Phone leads - USA banks I have variously...   \n",
       "104  110K Malaysian Online Casino Customers [ Depos...   \n",
       "105                                      PLEASE DELETE   \n",
       "\n",
       "                                                accept  \n",
       "0    [DATA LEAKS, CREDENTIALS OR ACCOUNTS, OFFERING...  \n",
       "1    [DATA LEAKS, COMPANY OR ORG INFORMATION, OFFER...  \n",
       "2    [CREDENTIALS OR ACCOUNTS, DATA LEAKS, OFFERING...  \n",
       "4                                             [ADVICE]  \n",
       "5    [OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...  \n",
       "..                                                 ...  \n",
       "101  [REQUEST FOR SERVICE OR PRODUCT, ADVICE, MALWA...  \n",
       "102  [VULNERABILITY, MALWARE TOOLS AND EXPLOITS, AD...  \n",
       "103  [OFFERING OF SERVICE OR PRODUCT, MONEY INVOLVE...  \n",
       "104  [OFFERING OF SERVICE OR PRODUCT, DATA LEAKS, P...  \n",
       "105               [DEFACEMENT, GOOD REVIEW, SCAM PAGE]  \n",
       "\n",
       "[252 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input relevant files to create test set\n",
    "file1 = \"forum_nulled_20230123_20230207_61_annotations_(test_set).jsonl\"\n",
    "file2 = \"forum_breached_20230123_20230207_102_annotations_(test_set).jsonl\"\n",
    "file3 = \"forum_xss_20230123_20230207_105_annotations_(test_set).jsonl\"\n",
    "\n",
    "test_set = [file1,file2,file3]\n",
    "\n",
    "df_dummy = merge_jsonl_files(test_set)\n",
    "df_dummy_dummy = df_dummy[df_dummy.answer == \"accept\"]\n",
    "test_df = df_dummy_dummy.drop(columns=[\"_input_hash\",\"_session_id\",\"_task_hash\",\"_view_id\",\"options\",\"config\", \"answer\"])\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(url_remover)\n",
    "\n",
    "del test_df[\"meta\"]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test_df[\"predicted_output\"] = test_df[\"text\"].apply(lambda text: nlp_predict(text, trained_nlp,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seb/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/seb/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/seb/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_set = mlb.fit_transform(test_df[\"predicted_output\"])\n",
    "y_test_set = mlb.fit_transform(test_df[\"accept\"])\n",
    "\n",
    "y_pred_set.shape == y_test_set.shape\n",
    "\n",
    "confusion_matrix_= multilabel_confusion_matrix(y_test_set, y_pred_set)\n",
    "cls_report = classification_report(y_test_set, y_pred_set)\n",
    "f1 = f1_score(y_test_set, y_pred_set, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>accept</th>\n",
       "      <th>predicted_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x2000 Steam Accounts with Games #4  \\n\\nThis l...</td>\n",
       "      <td>[DATA LEAKS, CREDENTIALS OR ACCOUNTS, OFFERING...</td>\n",
       "      <td>[CREDENTIALS OR ACCOUNTS, DATA LEAKS, VULNERAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332K Combolist EDU OFFICE 332K Combolist EDU O...</td>\n",
       "      <td>[DATA LEAKS, COMPANY OR ORG INFORMATION, OFFER...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mycanal ACCOUNTS PREMIUM diariatouaidara1999@g...</td>\n",
       "      <td>[CREDENTIALS OR ACCOUNTS, DATA LEAKS, OFFERING...</td>\n",
       "      <td>[CREDENTIALS OR ACCOUNTS, DATA LEAKS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Connecting to shoutbox Anyone have solution to...</td>\n",
       "      <td>[ADVICE]</td>\n",
       "      <td>[ADVICE, NETWORK OR PANEL ACCESS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BWW - Free Food - Accounts with Over 1000 Pts ...</td>\n",
       "      <td>[OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...</td>\n",
       "      <td>[CREDENTIALS OR ACCOUNTS, DATA LEAKS]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  x2000 Steam Accounts with Games #4  \\n\\nThis l...   \n",
       "1  332K Combolist EDU OFFICE 332K Combolist EDU O...   \n",
       "2  Mycanal ACCOUNTS PREMIUM diariatouaidara1999@g...   \n",
       "4  Connecting to shoutbox Anyone have solution to...   \n",
       "5  BWW - Free Food - Accounts with Over 1000 Pts ...   \n",
       "\n",
       "                                              accept  \\\n",
       "0  [DATA LEAKS, CREDENTIALS OR ACCOUNTS, OFFERING...   \n",
       "1  [DATA LEAKS, COMPANY OR ORG INFORMATION, OFFER...   \n",
       "2  [CREDENTIALS OR ACCOUNTS, DATA LEAKS, OFFERING...   \n",
       "4                                           [ADVICE]   \n",
       "5  [OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...   \n",
       "\n",
       "                                    predicted_output  \n",
       "0  [CREDENTIALS OR ACCOUNTS, DATA LEAKS, VULNERAB...  \n",
       "1  [OFFERING OF SERVICE OR PRODUCT, CREDENTIALS O...  \n",
       "2              [CREDENTIALS OR ACCOUNTS, DATA LEAKS]  \n",
       "4                  [ADVICE, NETWORK OR PANEL ACCESS]  \n",
       "5              [CREDENTIALS OR ACCOUNTS, DATA LEAKS]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7374784110535406\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.95      0.81        40\n",
      "           1       0.77      0.85      0.81        93\n",
      "           2       0.90      0.86      0.88        91\n",
      "           3       0.98      0.62      0.76        71\n",
      "           4       0.52      0.65      0.58        17\n",
      "           5       0.77      0.78      0.77        46\n",
      "           6       1.00      0.17      0.29         6\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.86      0.69      0.77        83\n",
      "           9       0.90      0.70      0.79        27\n",
      "          10       0.36      0.16      0.22        31\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.87      0.53      0.66        51\n",
      "          13       0.60      0.50      0.55         6\n",
      "          14       0.56      0.56      0.56         9\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.50      0.67      0.57         3\n",
      "          17       0.75      0.75      0.75         4\n",
      "          18       1.00      0.25      0.40        12\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       1.00      0.56      0.72        16\n",
      "          21       1.00      0.50      0.67         8\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       1.00      0.75      0.86         4\n",
      "\n",
      "   micro avg       0.81      0.68      0.74       628\n",
      "   macro avg       0.63      0.48      0.52       628\n",
      "weighted avg       0.81      0.68      0.72       628\n",
      " samples avg       0.69      0.62      0.63       628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f4ae5743a3c975258dd28a1e83587b05345ccc825b0fe1b5846a6cb0700706e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
